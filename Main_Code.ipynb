{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################### Organization ################################\n",
    "#############################################################################\n",
    "\n",
    "## Import some important libraries\n",
    "import os, sys, json, time, cv2, skimage, numpy as np, tensorflow as tf, matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "## Define root directory of the project\n",
    "ROOT_DIR = \".\"\n",
    "print(\"Check root directory: \", os.path.exists(ROOT_DIR), \"\\n\") \n",
    "assert os.path.exists(ROOT_DIR), \"ROOT_DIR does not exist\"\n",
    "\n",
    "## Import some important libraries of Mask R-CNN model \n",
    "sys.path.append(ROOT_DIR)\n",
    "from config import Config\n",
    "import utils as utils\n",
    "import visualize\n",
    "import model as modellib\n",
    "from model import log\n",
    "import dataset_preparation\n",
    "\n",
    "## CPU and GPU verification (numbers) \n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "## Make a directory to save logs, structure of trained model, and trained weights files in h5 format\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"Logs\")\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_RCNN.h5\")\n",
    "\n",
    "## Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############################# Model Configuration ###########################\n",
    "#############################################################################\n",
    "\n",
    "class CocoSynthConfig(Config):\n",
    "    \"\"\"Configuration for training on the box_synthetic dataset. Derives from the base Config class and overrides specific values.\n",
    "    \"\"\"\n",
    "    ## Give the configuration a recognizable name\n",
    "    NAME = \"Hadi_33_\"\n",
    "\n",
    "    ## Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    ## Number of classes (including background)\n",
    "    NUM_CLASSES = 4    # 1 background + 3 cell types\n",
    "\n",
    "    ## The size of training images originally was 512x512 in Mask R-CNN, However, we changed it to 64*64\n",
    "    IMAGE_MIN_DIM = 960\n",
    "    IMAGE_MAX_DIM = 960\n",
    "    \n",
    "    ## The confidence level (IoU) is 70% \n",
    "    DETECTION_MIN_CONFIDENCE = 0.70\n",
    "\n",
    "    ## You can experiment with this number to see if it improves training. It is better to be 1000, but slower. \n",
    "    STEPS_PER_EPOCH = 1      \n",
    "\n",
    "    ## This is how often validation is run. \n",
    "    ## If you are using too much hard drive space on saved models (in the MODEL_DIR), try making this value larger.\n",
    "    VALIDATION_STEPS = 1\n",
    "    \n",
    "    ## Backbone of the CNN model: \"resnet50\", \"resnet101\"  \n",
    "    BACKBONE = \"resnet101\"\n",
    "    IMAGE_RESIZE_MODE = \"hadi_crop\"\n",
    "\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "    MAX_GT_INSTANCES = 50 \n",
    "    POST_NMS_ROIS_INFERENCE = 500 \n",
    "    POST_NMS_ROIS_TRAINING = 1000 \n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "    \n",
    "    #############################################################################\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "    TRAIN_BN = False\n",
    "    #############################################################################\n",
    "    \n",
    "config = CocoSynthConfig()\n",
    "# config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################# Datasets ##################################\n",
    "#############################################################################\n",
    "\n",
    "## Load train dataset (images and masks)\n",
    "## I used the VIA 2.0.8 (VGG Image Annotator) software to annotate images, based on COCO standard format.\n",
    "dataset_train = dataset_preparation.CocoLikeDataset()\n",
    "dataset_train.load_data(\"Datasets/Phase1/train/Phase1_train.json\", \"Datasets/Phase1/train/images\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "## Load validation dataset (images and masks)\n",
    "dataset_val = dataset_preparation.CocoLikeDataset()\n",
    "dataset_val.load_data(\"Datasets/Phase1/val/Phase1_val.json\", \"Datasets/Phase1/val/images\")\n",
    "dataset_val.prepare()\n",
    "\n",
    "## Print name of categories in the train dataset. The BG category refers BACKGROUND.\n",
    "for name in [(\"Training\", dataset_train)]:\n",
    "    print(\"Categories:\\n_________________\")\n",
    "    for i, info in enumerate(dataset_train.class_info):\n",
    "        print(\"{:3}. {:50}\".format(i, info[\"name\"]))\n",
    "\n",
    "## Showing n samples from Train and Validation dataset.\n",
    "number_of_samples = 0                    # number of samples in each group\n",
    "for name, dataset in [(\"Training\", dataset_train), (\"Validation\", dataset_val)]:\n",
    "    print(\"=================>\", \"Number of\", name, \"images:\\t\", \"{}\".format(len(dataset.image_ids)))\n",
    "    image_ids = dataset.image_ids[0 : number_of_samples]   \n",
    "    for image_id in image_ids:\n",
    "        image = dataset.load_image(image_id)\n",
    "        mask, class_ids = dataset.load_mask(image_id)\n",
    "        visualize.display_top_masks(image, mask, class_ids, dataset.class_names)\n",
    "        bbox = utils.extract_bboxes(mask)\n",
    "        print(\"image_id \", image_id, dataset.image_reference(image_id))\n",
    "        log(\"image\", image)\n",
    "        log(\"mask\", mask)\n",
    "        log(\"class_ids\", class_ids)\n",
    "        log(\"bbox\", bbox)\n",
    "        visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, size_of_gt =len(bbox), figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################## Model initializing ##################\n",
    "########################################################\n",
    "\n",
    "model = modellib.MaskRCNN(mode = \"training\", config = config, model_dir = MODEL_DIR)\n",
    "\n",
    "## Which weights to start with?\n",
    "init_with = \"imagenet\"           # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name = True)\n",
    "elif init_with == \"coco\":\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name = True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    ## Load the last model you trained and continue training\n",
    "    model_path = str(Path(ROOT_DIR)/\"Logs\"/\"hadi_33_20190819T1103/mask_rcnn_hadi_33__1200.h5\")\n",
    "    assert model_path != \"\", \"Provide path to trained weights\"\n",
    "    model.load_weights(model_path, by_name = True)\n",
    "\n",
    "###################### Train ######################\n",
    "###################################################\n",
    "\n",
    "start_train = time.time()\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE,    # fine train:     learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=200,\n",
    "            layers=\"all\")\n",
    "end_train = time.time()\n",
    "hours = round((end_train - start_train) / 1, 2)\n",
    "print(\"\\n\", f\"Training took {hours} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################### Test ######################\n",
    "##################################################\n",
    "\n",
    "class InferenceConfig(CocoSynthConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    IMAGE_MIN_DIM = 960\n",
    "    IMAGE_MAX_DIM = 960\n",
    "    DETECTION_MIN_CONFIDENCE = 0.70\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "## Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode = \"inference\", config = inference_config, model_dir = MODEL_DIR)\n",
    "## Get path to saved weights (Either set a specific path or find last trained weights)\n",
    "# model_path = str(Path(ROOT_DIR)/\"Logs/hadi_33_20190819T1103/mask_rcnn_hadi_33__1200.h5\")    # trained by 512*512 images\n",
    "model_path = str(Path(ROOT_DIR)/\"Logs/hadi_33_20190912T1518/mask_rcnn_hadi_33__1102.h5\")    # trained by 960*960 images\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name = True)\n",
    "\n",
    "##################################################\n",
    "\n",
    "real_test_dir = \"Datasets/Phase1/test/images\"\n",
    "image_paths = []\n",
    "image_id = 0\n",
    "dataset_val = dataset_preparation.CocoLikeDataset()\n",
    "dataset_val.load_data(\"Datasets/Phase1/test/Phase1_test.json\", \"Datasets/Phase1/test/images\")\n",
    "dataset_val.prepare()\n",
    "print(\"=================================\")\n",
    "for filename in os.listdir(real_test_dir):\n",
    "    print(filename)\n",
    "    if os.path.splitext(filename)[1].lower() in [\".png\", \".jpg\", \".jpeg\", \".tif\"]:\n",
    "        image_paths.append(os.path.join(real_test_dir, filename))\n",
    "for image_path in image_paths:\n",
    "    img = skimage.io.imread(image_path)\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    img_arr = np.array(img)\n",
    "    results = model.detect([img_arr], verbose = 0)\n",
    "    Result_pred = results[0]\n",
    "#     for i in Result_pred:\n",
    "#         print(i, Result_pred[i].shape)\n",
    "    print(\"\\n#################################### Detections ####################################\\n\")\n",
    "    print(\"Predicted class_ids =\\n\", Result_pred[\"scores\"])\n",
    "    visualize.display_instances(img, Result_pred[\"rois\"], Result_pred[\"masks\"], Result_pred[\"class_ids\"],\n",
    "                                dataset_train.class_names, Result_pred[\"scores\"], title = \"Detections\", figsize = (10,10),\n",
    "                                edgecolor = \"red\", size_of_gt = len(Result_pred[\"rois\"]))\n",
    "    print(\"#################################### Ground Truth #########################################\")\n",
    "    image = dataset_val.load_image(image_id)\n",
    "    mask, class_ids = dataset_val.load_mask(image_id)\n",
    "    bbox = utils.extract_bboxes(mask)\n",
    "    gt_box = bbox\n",
    "    gt_mask = mask\n",
    "    gt_class_id = class_ids\n",
    "    visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, title = \"Ground Truth\", \n",
    "                                figsize = (10,10), edgecolor = \"darkgreen\", size_of_gt = len(gt_box))\n",
    "    image_id = image_id + 1\n",
    "    print(\"###################################### Mix ############################################\")\n",
    "    visualize.display_differences(img, gt_box, gt_class_id, gt_mask, Result_pred[\"rois\"], Result_pred[\"class_ids\"], \n",
    "                                  Result_pred[\"scores\"], Result_pred[\"masks\"], dataset_train.class_names, \n",
    "                                  title = \"Ground Truth and Derection\", ax = None, show_mask = True, show_box = True, \n",
    "                                  iou_threshold = 0.5, score_threshold = 0.5, size_of_gt =len(gt_box), edgecolor = \"darkgreen\")\n",
    "####################################### mAP ##########################################\n",
    "    Result_metrics = utils.compute_ap(gt_box, gt_class_id, gt_mask, Result_pred[\"rois\"], Result_pred[\"class_ids\"], \n",
    "                                      Result_pred[\"scores\"], Result_pred[\"masks\"], iou_threshold = 0.7)\n",
    "#     print(\"********* P =\", np.mean(Result_metrics[1][1:-2]))\n",
    "#     print(\"********* R =\", np.mean(Result_metrics[2][1:-2]))\n",
    "#     print(\"********* F1-score =\", (2 * P * R) / (P + R), \"\\n\")\n",
    "#     print(\"********* Precision =\", Result_metrics[1].shape, \"\\n\", Result_metrics[1])\n",
    "#     print(\"********* Recall =\", Result_metrics[2].shape, \"\\n\", Result_metrics[2])\n",
    "#     print(\"********* Overlaps =\", Result_metrics[3].shape, \"\\n\", Result_metrics[3])\n",
    "    print(\"********* mAP =\", Result_metrics[0])\n",
    "    visualize.plot_precision_recall(Result_metrics[0], Result_metrics[1], Result_metrics[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############################### Confusion Matrix #####################################\n",
    "######################################################################################\n",
    "\n",
    "print(__doc__)\n",
    "################################# Manually counted ###################################\n",
    "# GT_N11 = np.array([0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2])\n",
    "# Detection_N11 = np.array([1,2,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,3])\n",
    "# GT_C11 = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#                    2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3])\n",
    "# Detection_C11 = np.array([0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#                           0,1,1,1,1,2,3,3,3,3,3,3,0,0,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3])\n",
    "GT_P11 = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3])\n",
    "Detection_P11 = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,3,3,3,3,2,3,3,3,3,3,3,3])\n",
    "######################################################################################\n",
    "\n",
    "y_test = GT_P11\n",
    "y_pred = Detection_P11\n",
    "print(y_test.shape, y_pred.shape)\n",
    "class_names = [\"Live\", \"Intermediate\", \"    Pyknotic\"]\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Greens):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = \"Confusion matrix (Normalized)\"\n",
    "        else:\n",
    "            title = \"Confusion matrix\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis = 1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    im = ax.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n",
    "    ax.figure.colorbar(im, ax = ax)\n",
    "    \n",
    "    plt.title(title, fontsize = 30, pad = 50, fontweight = \"bold\", color = \"blue\")\n",
    "    \n",
    "    ax.set_xlabel(\"Predicted counts\", fontsize = 25, fontweight = \"bold\")\n",
    "    ax.set_ylabel(\"True counts\", fontsize = 25, fontweight = \"bold\")\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    ax.xaxis.set_label_coords(0.5, 1.25)\n",
    "    ax.yaxis.set_label_coords(-0.5, 0.5)\n",
    "\n",
    "#     plt.setp(ax.get_xticklabels(), rotation = 30, ha = \"right\", va = \"center\", rotation_mode = \"anchor\", style=\"italic\")\n",
    "    ax.set(xticks = np.arange(cm.shape[1]), yticks = np.arange(cm.shape[0]))\n",
    "    ax.xaxis.tick_top() \n",
    "    ax.set_xticklabels(classes, size = 15, fontweight = \"bold\", color = \"blue\")\n",
    "    ax.set_yticklabels(classes, size = 15, fontweight = \"bold\", color = \"blue\")\n",
    "    ax.xaxis.set_tick_params(color=\"darkblue\", width = 3, length = 8)\n",
    "    ax.yaxis.set_tick_params(color=\"darkblue\", width = 3, length = 8)\n",
    "\n",
    "    fmt = \".3f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), dict(size = 18), ha = \"center\", va = \"center\", \n",
    "                    fontname = \"Arial\", fontweight = \"bold\", color = \"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.tight_layout(pad = 200, h_pad = 100, w_pad = 10, rect = (0,0,0.5,0.5))\n",
    "    return ax\n",
    "\n",
    "np.set_printoptions(precision = 3)\n",
    "plot_confusion_matrix(y_test, y_pred, classes = class_names, title = \"Confusion matrix\")\n",
    "plot_confusion_matrix(y_test, y_pred, classes = class_names, normalize = True, title = \"Confusion matrix (Normalized)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Precision =\\t\", precision_score(y_test, y_pred, average = \"weighted\"))\n",
    "print(\"Recall =\\t\", recall_score(y_test, y_pred, average = \"weighted\"))\n",
    "print(\"F1_score =\\t\", f1_score(y_test, y_pred, average = \"weighted\"))\n",
    "print(\"Accuracy =\\t\", accuracy_score(y_test, y_pred, normalize = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cos)",
   "language": "python",
   "name": "cos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
