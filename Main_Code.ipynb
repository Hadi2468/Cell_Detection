{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "##############   Step 1: Organization and Model Configuration  ##############\n",
    "#############################################################################\n",
    "\n",
    "### Import some important libraries\n",
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "import os, sys, json, time, cv2, skimage, numpy as np, tensorflow as tf, matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "### Define root directory of the project\n",
    "# ROOT_DIR = \".\"       # Github\n",
    "ROOT_DIR = \"../\"\n",
    "# print(\"Check root directory: \", os.path.exists(ROOT_DIR), \"\\n\") \n",
    "assert os.path.exists(ROOT_DIR), \"ROOT_DIR does not exist\"\n",
    "\n",
    "### Import some important libraries of Mask R-CNN model \n",
    "sys.path.append(ROOT_DIR)\n",
    "# from config import Config       # Github\n",
    "# import utils as utils       # Github\n",
    "# import visualize       # Github\n",
    "# import model as modellib       # Github\n",
    "# from model import log       # Github\n",
    "# import dataset_preparation       # Github\n",
    "from mrcnn.config import Config\n",
    "import mrcnn.utils as utils\n",
    "from mrcnn import visualize\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "from mrcnn import dataset_preparation\n",
    "\n",
    "### CPU and GPU verification (numbers) \n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "### Make a directory to save logs, structure of trained model, and trained weights files in h5 format\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"Logs\")\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_RCNN.h5\")\n",
    "\n",
    "### Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "    \n",
    "# import imgaug as ia\n",
    "# import imgaug.augmenters as iaa\n",
    "\n",
    "######################### Model Configuration ######################  \n",
    "class CocoSynthConfig(Config):\n",
    "    \"\"\"Configuration for training on the box_synthetic dataset. Derives from the base Config class and overrides specific values.\n",
    "    \"\"\"\n",
    "    ### Give the configuration a recognizable name\n",
    "    NAME = \"Hadi_71_\"\n",
    "\n",
    "    ### Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 3\n",
    "\n",
    "    ## Number of classes (including background)\n",
    "    NUM_CLASSES = 4    # 1 background + 3 cell types\n",
    "\n",
    "    ### The size of training images originally was 512x512 in Mask R-CNN, However, we changed it to 64*64\n",
    "    IMAGE_MIN_DIM = 512 # 64, 512, 640, 960 \n",
    "    IMAGE_MAX_DIM = 512 # 64, 512, 640, 960 \n",
    "    \n",
    "    ### The confidence level (IoU) is 70% \n",
    "    DETECTION_MIN_CONFIDENCE = 0.70\n",
    "\n",
    "    ### You can experiment with this number to see if it improves training. It is better to be 1000, but slower. \n",
    "    STEPS_PER_EPOCH = 10000      \n",
    "\n",
    "    ### This is how often validation is run. \n",
    "    ### If you are using too much hard drive space on saved models (in the MODEL_DIR), try making this value larger.\n",
    "    VALIDATION_STEPS = 10\n",
    "    \n",
    "    ### Backbone of the CNN model: \"resnet50\", \"resnet101\"  \n",
    "    BACKBONE = \"resnet101\"\n",
    "    IMAGE_RESIZE_MODE = \"hadi_crop\"\n",
    "\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "    MAX_GT_INSTANCES = 50 \n",
    "    POST_NMS_ROIS_INFERENCE = 500 \n",
    "    POST_NMS_ROIS_TRAINING = 1000 \n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "    \n",
    "    ###################################################################################\n",
    "    ### If enabled, resizes instance masks to a smaller size to reduce memory load. ### \n",
    "    ### Recommended when using high-resolution images.                              ###\n",
    "    USE_MINI_MASK = True                                                            ###\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask                  ###\n",
    "    TRAIN_BN = False            # Hadi: True train_BN causes the loss increase      ###\n",
    "    ###################################################################################\n",
    "    \n",
    "config = CocoSynthConfig()\n",
    "# config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "###########################   Step 2: Datasets   ############################\n",
    "#############################################################################\n",
    "\n",
    "################### Improving json files of second data ######################################\n",
    "# import pandas as pd\n",
    "# Annotate = pd.read_json(\"/Users/drhad/--Deep1--/JSON/Hadi_XiangDi_COCO.json\")\n",
    "# Annotate\n",
    "# df = pd.read_json((Annotate['annotations']).to_json(), orient='index')\n",
    "# df = df.rename(index=lambda s: s + 2000)\n",
    "# df = df.rename(index=lambda s: '\"id\": ' + str(s))\n",
    "# df = df.drop(columns=\"id\")\n",
    "# df.to_json(r'C:\\Users\\drhad\\--Deep1--\\JSON\\Hadi_XiangDi_COCO_2000.json', orient='index')\n",
    "##############################################################################################\n",
    "\n",
    "### Load train and validation dataset (images and masks). \n",
    "### I used VIA 2.0.8 (VGG Image Annotator) software for annotation based on COCO format.\n",
    "# os.getcwd()\n",
    "dataset_train = dataset_preparation.CocoLikeDataset()\n",
    "dataset_train.load_data(\"../Datasets/Hadi_71/Hadi_71_train.json\", \"../Datasets/Hadi_71/71_images\")\n",
    "dataset_train.prepare()\n",
    "dataset_val = dataset_preparation.CocoLikeDataset()\n",
    "dataset_val.load_data(\"../Datasets/Hadi_71/Hadi_71_valid.json\", \"../Datasets/Hadi_71/71_images\")\n",
    "dataset_val.prepare()\n",
    "\n",
    "# Print name of categories in the train dataset. The BG category refers BACKGROUND.\n",
    "# for name in [(\"Training\", dataset_train)]:\n",
    "#     print(\"Categories:\\n_________________\")\n",
    "#     for i, info in enumerate(dataset_train.class_info):\n",
    "#         print(\"{:3}. {:50}\".format(i, info[\"name\"]))\n",
    "\n",
    "### Showing n samples from Train and Validation dataset.\n",
    "number_of_samples = 0            # number of samples in each group: 38\n",
    "# for name, dataset in [(\"Training\", dataset_train), (\"Validation\", dataset_val)]:     # Train & Valid\n",
    "for name, dataset in [(\"Training\", dataset_train)]:                                    # Train\n",
    "    print(\"____________________\", \"Number of\", name, \"images:\", \"{}\".format(len(dataset.image_ids)), \"____________________\")\n",
    "    image_ids = dataset.image_ids[0 : number_of_samples]   \n",
    "    for image_id in image_ids:\n",
    "        image = dataset.load_image(image_id)\n",
    "        mask, class_ids = dataset.load_mask(image_id)\n",
    "#         visualize.display_top_masks(image, mask, class_ids, dataset.class_names)\n",
    "        bbox = utils.extract_bboxes(mask)\n",
    "        print(\"image_id:\", image_id, dataset.image_reference(image_id), \"--> \", dataset.images_names[image_id])\n",
    "#         log(\"image\", image)\n",
    "#         log(\"mask\", mask)\n",
    "#         log(\"class_ids\", class_ids)\n",
    "#         log(\"bbox\", bbox)\n",
    "        plt.figure(figsize=(7,7))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "        visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, size_of_gt =len(bbox), figsize=(7,7))\n",
    "        \n",
    "### Uncomment the scripts in the mrcnn.Visulize (I have changed to print original and masked images to compare with together)\n",
    "###    aaaa = class_ids.tolist()                                                     # Hadi\n",
    "###    print(\"\\n***** Live:\\t\\t\", aaaa.count(3))                                     # Hadi\n",
    "###    print(\"***** Intermediate:\\t\", aaaa.count(2))                                 # Hadi\n",
    "###    print(\"***** Dead:\\t\\t\", aaaa.count(1))                                       # Hadi\n",
    "###    print(\"***** Total counted cells: \", N)                                       # Hadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#####################   Step 3: Model initializing   ########################\n",
    "#############################################################################\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "\n",
    "### Which weights to start with?\n",
    "init_with = \"imagenet\"           # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name = True)\n",
    "elif init_with == \"coco\":\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name = True, \n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    ### Load the last model you trained and continue training\n",
    "    model_path = str(Path(ROOT_DIR)/\"Logs/hadi_71_20200408T0922/mask_rcnn_hadi_71__1000.h5\")     # 512*512 (batch size: 3)\n",
    "    assert model_path != \"\", \"Provide path to trained weights\"\n",
    "    model.load_weights(model_path, by_name = True)\n",
    "\n",
    "###################################################################\n",
    "#####################   Step 5: Training   ########################\n",
    "###################################################################\n",
    "\n",
    "# augmentation = iaa.Sometimes(0.9, [iaa.Fliplr(0.5), iaa.Flipud(0.5), iaa.Multiply((0.8, 1.2)), \n",
    "#                                    iaa.GaussianBlur(sigma=(0.0, 5.0))])\n",
    "\n",
    "drop_rate = 1.0       # (0.0, 0.1, ..., 0.9, 1.0)  where 1.0 means no dropout, and 0.0 means no outputs from the layer\n",
    "\n",
    "start_train = time.time()\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE,    # fine train:     learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs = 2000,                  # epochs = 3000\n",
    "            in_drop_rate  = drop_rate,\n",
    "            hid_drop_rate = drop_rate,\n",
    "            out_drop_rate = drop_rate,\n",
    "#             augmentation=augmentation,\n",
    "            layers=\"all\")\n",
    "end_train = time.time()\n",
    "hours = round((end_train - start_train) / 1, 2)\n",
    "print(\"\\n\", f\"Training took {hours} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################### Test ######################\n",
    "##################################################\n",
    "\n",
    "class InferenceConfig(CocoSynthConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    IMAGE_MIN_DIM = 512 # 64, 640, 960 \n",
    "    IMAGE_MAX_DIM = 512 # 64, 640, 960 \n",
    "    DETECTION_MIN_CONFIDENCE = 0.70\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "## Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode = \"inference\", config = inference_config, model_dir = MODEL_DIR)\n",
    "## Get path to saved weights (Either set a specific path or find last trained weights)\n",
    "# model_path = str(Path(ROOT_DIR)/\"Logs/hadi_33_20190819T1103/mask_rcnn_hadi_33__1200.h5\")    # trained by 512*512 images\n",
    "model_path = str(Path(ROOT_DIR)/\"Logs/hadi_33_20190912T1518/mask_rcnn_hadi_33__1102.h5\")    # trained by 960*960 images\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name = True)\n",
    "\n",
    "##################################################\n",
    "\n",
    "real_test_dir = \"Datasets/Phase1/test/images\"\n",
    "image_paths = []\n",
    "image_id = 0\n",
    "dataset_val = dataset_preparation.CocoLikeDataset()\n",
    "dataset_val.load_data(\"Datasets/Phase1/test/Phase1_test.json\", \"Datasets/Phase1/test/images\")\n",
    "dataset_val.prepare()\n",
    "print(\"=================================\")\n",
    "for filename in os.listdir(real_test_dir):\n",
    "    print(filename)\n",
    "    if os.path.splitext(filename)[1].lower() in [\".png\", \".jpg\", \".jpeg\", \".tif\"]:\n",
    "        image_paths.append(os.path.join(real_test_dir, filename))\n",
    "for image_path in image_paths:\n",
    "    img = skimage.io.imread(image_path)\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    img_arr = np.array(img)\n",
    "    results = model.detect([img_arr], verbose = 0)\n",
    "    Result_pred = results[0]\n",
    "#     for i in Result_pred:\n",
    "#         print(i, Result_pred[i].shape)\n",
    "    print(\"\\n#################################### Detections ####################################\\n\")\n",
    "    print(\"Predicted class_ids =\\n\", Result_pred[\"scores\"])\n",
    "    visualize.display_instances(img, Result_pred[\"rois\"], Result_pred[\"masks\"], Result_pred[\"class_ids\"],\n",
    "                                dataset_train.class_names, Result_pred[\"scores\"], title = \"Detections\", figsize = (10,10),\n",
    "                                edgecolor = \"red\", size_of_gt = len(Result_pred[\"rois\"]))\n",
    "    print(\"#################################### Ground Truth #########################################\")\n",
    "    image = dataset_val.load_image(image_id)\n",
    "    mask, class_ids = dataset_val.load_mask(image_id)\n",
    "    bbox = utils.extract_bboxes(mask)\n",
    "    gt_box = bbox\n",
    "    gt_mask = mask\n",
    "    gt_class_id = class_ids\n",
    "    visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, title = \"Ground Truth\", \n",
    "                                figsize = (10,10), edgecolor = \"darkgreen\", size_of_gt = len(gt_box))\n",
    "    image_id = image_id + 1\n",
    "    print(\"###################################### Mix ############################################\")\n",
    "    visualize.display_differences(img, gt_box, gt_class_id, gt_mask, Result_pred[\"rois\"], Result_pred[\"class_ids\"], \n",
    "                                  Result_pred[\"scores\"], Result_pred[\"masks\"], dataset_train.class_names, \n",
    "                                  title = \"Ground Truth and Derection\", ax = None, show_mask = True, show_box = True, \n",
    "                                  iou_threshold = 0.5, score_threshold = 0.5, size_of_gt =len(gt_box), edgecolor = \"darkgreen\")\n",
    "####################################### mAP ##########################################\n",
    "    Result_metrics = utils.compute_ap(gt_box, gt_class_id, gt_mask, Result_pred[\"rois\"], Result_pred[\"class_ids\"], \n",
    "                                      Result_pred[\"scores\"], Result_pred[\"masks\"], iou_threshold = 0.7)\n",
    "#     print(\"********* P =\", np.mean(Result_metrics[1][1:-2]))\n",
    "#     print(\"********* R =\", np.mean(Result_metrics[2][1:-2]))\n",
    "#     print(\"********* F1-score =\", (2 * P * R) / (P + R), \"\\n\")\n",
    "#     print(\"********* Precision =\", Result_metrics[1].shape, \"\\n\", Result_metrics[1])\n",
    "#     print(\"********* Recall =\", Result_metrics[2].shape, \"\\n\", Result_metrics[2])\n",
    "#     print(\"********* Overlaps =\", Result_metrics[3].shape, \"\\n\", Result_metrics[3])\n",
    "    print(\"********* mAP =\", Result_metrics[0])\n",
    "    visualize.plot_precision_recall(Result_metrics[0], Result_metrics[1], Result_metrics[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############################### Confusion Matrix #####################################\n",
    "######################################################################################\n",
    "\n",
    "print(__doc__)\n",
    "################################# Manually counted ###################################\n",
    "# GT_N11 = np.array([0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2])\n",
    "# Detection_N11 = np.array([1,2,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,3])\n",
    "# GT_C11 = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#                    2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3])\n",
    "# Detection_C11 = np.array([0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#                           0,1,1,1,1,2,3,3,3,3,3,3,0,0,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3])\n",
    "GT_P11 = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3])\n",
    "Detection_P11 = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,3,3,3,3,2,3,3,3,3,3,3,3])\n",
    "######################################################################################\n",
    "\n",
    "y_test = GT_P11\n",
    "y_pred = Detection_P11\n",
    "print(y_test.shape, y_pred.shape)\n",
    "class_names = [\"Live\", \"Intermediate\", \"    Pyknotic\"]\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Greens):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = \"Confusion matrix (Normalized)\"\n",
    "        else:\n",
    "            title = \"Confusion matrix\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis = 1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    im = ax.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n",
    "    ax.figure.colorbar(im, ax = ax)\n",
    "    \n",
    "    plt.title(title, fontsize = 30, pad = 50, fontweight = \"bold\", color = \"blue\")\n",
    "    \n",
    "    ax.set_xlabel(\"Predicted counts\", fontsize = 25, fontweight = \"bold\")\n",
    "    ax.set_ylabel(\"True counts\", fontsize = 25, fontweight = \"bold\")\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    ax.xaxis.set_label_coords(0.5, 1.25)\n",
    "    ax.yaxis.set_label_coords(-0.5, 0.5)\n",
    "\n",
    "#     plt.setp(ax.get_xticklabels(), rotation = 30, ha = \"right\", va = \"center\", rotation_mode = \"anchor\", style=\"italic\")\n",
    "    ax.set(xticks = np.arange(cm.shape[1]), yticks = np.arange(cm.shape[0]))\n",
    "    ax.xaxis.tick_top() \n",
    "    ax.set_xticklabels(classes, size = 15, fontweight = \"bold\", color = \"blue\")\n",
    "    ax.set_yticklabels(classes, size = 15, fontweight = \"bold\", color = \"blue\")\n",
    "    ax.xaxis.set_tick_params(color=\"darkblue\", width = 3, length = 8)\n",
    "    ax.yaxis.set_tick_params(color=\"darkblue\", width = 3, length = 8)\n",
    "\n",
    "    fmt = \".3f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), dict(size = 18), ha = \"center\", va = \"center\", \n",
    "                    fontname = \"Arial\", fontweight = \"bold\", color = \"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.tight_layout(pad = 200, h_pad = 100, w_pad = 10, rect = (0,0,0.5,0.5))\n",
    "    return ax\n",
    "\n",
    "np.set_printoptions(precision = 3)\n",
    "plot_confusion_matrix(y_test, y_pred, classes = class_names, title = \"Confusion matrix\")\n",
    "plot_confusion_matrix(y_test, y_pred, classes = class_names, normalize = True, title = \"Confusion matrix (Normalized)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Precision =\\t\", precision_score(y_test, y_pred, average = \"weighted\"))\n",
    "print(\"Recall =\\t\", recall_score(y_test, y_pred, average = \"weighted\"))\n",
    "print(\"F1_score =\\t\", f1_score(y_test, y_pred, average = \"weighted\"))\n",
    "print(\"Accuracy =\\t\", accuracy_score(y_test, y_pred, normalize = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
